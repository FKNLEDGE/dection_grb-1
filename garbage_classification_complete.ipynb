{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# æ™ºèƒ½åƒåœ¾åˆ†ç±»ç³»ç»Ÿ - å®Œæ•´å®éªŒæµç¨‹\n",
    "# Intelligent Waste Classification System - Complete Workflow\n",
    "\n",
    "**åŸºäºè¿ç§»å­¦ä¹ å’ŒMobileNetV2çš„æ™ºèƒ½åƒåœ¾åˆ†ç±»ç³»ç»Ÿ**\n",
    "\n",
    "æœ¬NotebookåŒ…å«å®Œæ•´çš„å®éªŒæµç¨‹ï¼š\n",
    "- ğŸ“¦ ç¯å¢ƒé…ç½®\n",
    "- ğŸ“¥ æ•°æ®é›†ä¸‹è½½\n",
    "- ğŸ”§ æ•°æ®é¢„å¤„ç†\n",
    "- ğŸ§  æ¨¡å‹è®­ç»ƒï¼ˆMobileNetV2ã€ResNet50ã€VGG16ã€DenseNet121ã€EfficientNetB0ï¼‰\n",
    "- ğŸ“Š ç»“æœè¯„ä¼°ä¸å¯è§†åŒ–\n",
    "- ğŸ”¬ æ¶ˆèå®éªŒï¼ˆCBAMæ³¨æ„åŠ›æœºåˆ¶ + Focal Lossï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ç›®å½•\n",
    "1. [ç¯å¢ƒè®¾ç½®](#1-ç¯å¢ƒè®¾ç½®)\n",
    "2. [é¡¹ç›®å…‹éš†ä¸ä¾èµ–å®‰è£…](#2-é¡¹ç›®å…‹éš†ä¸ä¾èµ–å®‰è£…)\n",
    "3. [Kaggleæ•°æ®é›†ä¸‹è½½](#3-kaggleæ•°æ®é›†ä¸‹è½½)\n",
    "4. [æ•°æ®æ¢ç´¢](#4-æ•°æ®æ¢ç´¢)\n",
    "5. [æ¨¡å‹è®­ç»ƒ](#5-æ¨¡å‹è®­ç»ƒ)\n",
    "6. [ç»“æœè¯„ä¼°](#6-ç»“æœè¯„ä¼°)\n",
    "7. [æ¶ˆèå®éªŒï¼ˆå¯é€‰ï¼‰](#7-æ¶ˆèå®éªŒå¯é€‰)\n",
    "8. [ä¸‹è½½ç»“æœ](#8-ä¸‹è½½ç»“æœ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "æ£€æŸ¥GPUç¯å¢ƒï¼Œè®¾ç½®è¿è¡Œç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlowç‰ˆæœ¬:\", tf.__version__)\n",
    "print(\"GPUå¯ç”¨:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# è®¾ç½®GPUå†…å­˜å¢é•¿\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPUï¼Œå·²å¯ç”¨å†…å­˜å¢é•¿\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPUé…ç½®é”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. é¡¹ç›®å…‹éš†ä¸ä¾èµ–å®‰è£…\n",
    "\n",
    "ä»GitHubå…‹éš†é¡¹ç›®ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…‹éš†é¡¹ç›®ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "# æ–¹å¼1ï¼šä»GitHubå…‹éš†\n",
    "# !git clone https://github.com/FKNLEDGE/dection_grb-1.git\n",
    "# %cd dection_grb-1\n",
    "\n",
    "# æ–¹å¼2ï¼šå¦‚æœå·²ä¸Šä¼ æ–‡ä»¶åˆ°Colabï¼Œè·³è¿‡å…‹éš†æ­¥éª¤\n",
    "# ç¡®ä¿å½“å‰ç›®å½•åŒ…å«é¡¹ç›®æ–‡ä»¶\n",
    "\n",
    "import os\n",
    "# å¦‚æœåœ¨é¡¹ç›®ç›®å½•ä¸­ï¼Œæ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–åŒ…\n",
    "!pip install -q tensorflow>=2.10.0\n",
    "!pip install -q numpy pandas scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q Pillow opencv-python\n",
    "!pip install -q tqdm\n",
    "!pip install -q kaggle\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Kaggleæ•°æ®é›†ä¸‹è½½\n",
    "\n",
    "### 3.1 ä¸Šä¼ Kaggle API Token\n",
    "\n",
    "**è·å–Kaggle API Tokenæ­¥éª¤ï¼š**\n",
    "1. è®¿é—® [Kaggle](https://www.kaggle.com)\n",
    "2. ç‚¹å‡»å³ä¸Šè§’å¤´åƒ â†’ Settings\n",
    "3. æ‰¾åˆ° \"API\" éƒ¨åˆ†ï¼Œç‚¹å‡» \"Create New Token\"\n",
    "4. ä¸‹è½½ `kaggle.json` æ–‡ä»¶\n",
    "5. åœ¨ä¸‹æ–¹å•å…ƒæ ¼ä¸Šä¼ è¯¥æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸Šä¼ kaggle.jsonæ–‡ä»¶\n",
    "from google.colab import files\n",
    "\n",
    "print(\"è¯·ä¸Šä¼ æ‚¨çš„ kaggle.json æ–‡ä»¶ï¼š\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# é…ç½®Kaggle API\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\nâœ… Kaggle APIé…ç½®å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ä¸‹è½½åƒåœ¾åˆ†ç±»æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½æ•°æ®é›†\n",
    "!kaggle datasets download -d mostafaabla/garbage-classification\n",
    "\n",
    "# è§£å‹æ•°æ®é›†\n",
    "!unzip -q garbage-classification.zip -d ./data\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®é›†ç»“æ„\n",
    "import os\n",
    "\n",
    "# å¤„ç†å¯èƒ½çš„ç›®å½•ç»“æ„å·®å¼‚\n",
    "if os.path.exists('./data/Garbage classification'):\n",
    "    !mv './data/Garbage classification' ./data/garbage_classification\n",
    "elif os.path.exists('./data/garbage_classification'):\n",
    "    print(\"æ•°æ®é›†ç›®å½•å·²å­˜åœ¨\")\n",
    "else:\n",
    "    # å°è¯•æŸ¥æ‰¾æ•°æ®é›†ä½ç½®\n",
    "    !find ./data -type d -maxdepth 2\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®é›†ç±»åˆ«\n",
    "data_dir = './data/garbage_classification'\n",
    "if os.path.exists(data_dir):\n",
    "    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    print(f\"\\nâœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼å…± {len(classes)} ä¸ªç±»åˆ«ï¼š\")\n",
    "    for i, cls in enumerate(classes, 1):\n",
    "        count = len(os.listdir(os.path.join(data_dir, cls)))\n",
    "        print(f\"  {i}. {cls}: {count} å¼ å›¾ç‰‡\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®é›†ç›®å½•æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è§£å‹è·¯å¾„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. æ•°æ®æ¢ç´¢\n",
    "\n",
    "å¯è§†åŒ–æ•°æ®é›†æ ·æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "data_dir = './data/garbage_classification'\n",
    "classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„ä¸€å¼ ç¤ºä¾‹å›¾ç‰‡\n",
    "fig, axes = plt.subplots(3, 4, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, cls in enumerate(classes):\n",
    "    cls_dir = os.path.join(data_dir, cls)\n",
    "    images = [f for f in os.listdir(cls_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if images:\n",
    "        sample_img = random.choice(images)\n",
    "        img_path = os.path.join(cls_dir, sample_img)\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"{cls}\\n({len(images)} images)\", fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… æ•°æ®é›†æ ·æœ¬å¯è§†åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "### 5.1 å¿«é€Ÿæµ‹è¯•ï¼ˆ3ä¸ªepochï¼‰\n",
    "\n",
    "å…ˆç”¨å°‘é‡epochå¿«é€Ÿæµ‹è¯•ä»£ç æ˜¯å¦æ­£å¸¸è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿«é€Ÿæµ‹è¯•æ¨¡å¼\n",
    "from main import run_quick_test\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å¼€å§‹å¿«é€Ÿæµ‹è¯•ï¼ˆ3 epochsï¼Œä»…MobileNetV2ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    results, models, histories = run_quick_test(\n",
    "        data_dir='./data/garbage_classification',\n",
    "        epochs=3,\n",
    "        models=['MobileNetV2']\n",
    "    )\n",
    "    print(\"\\nâœ… å¿«é€Ÿæµ‹è¯•æˆåŠŸï¼ä»£ç è¿è¡Œæ­£å¸¸ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å®Œæ•´è®­ç»ƒï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "è¿è¡Œå®Œæ•´çš„å¤šæ¨¡å‹å¯¹æ¯”å®éªŒï¼ˆ30ä¸ªepochï¼‰\n",
    "\n",
    "**âš ï¸ æ³¨æ„ï¼šå®Œæ•´è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆçº¦1-3å°æ—¶ï¼‰ï¼Œå»ºè®®åœ¨æœ‰GPUçš„ç¯å¢ƒä¸‹è¿è¡Œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´è®­ç»ƒ - å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥è¿è¡Œ\n",
    "# from main import run_all_experiments\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"å¼€å§‹å®Œæ•´è®­ç»ƒï¼ˆ30 epochsï¼Œ5ä¸ªæ¨¡å‹ï¼‰\")\n",
    "# print(\"é¢„è®¡æ—¶é—´ï¼š1-3å°æ—¶ï¼ˆå–å†³äºGPUæ€§èƒ½ï¼‰\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# try:\n",
    "#     results, models, histories = run_all_experiments(\n",
    "#         data_dir='./data/garbage_classification',\n",
    "#         epochs=30\n",
    "#     )\n",
    "#     print(\"\\nâœ… å®Œæ•´è®­ç»ƒæˆåŠŸï¼\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nâŒ è®­ç»ƒå¤±è´¥: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "print(\"æç¤ºï¼šå¦‚éœ€è¿è¡Œå®Œæ•´è®­ç»ƒï¼Œè¯·å–æ¶ˆä¸Šæ–¹å•å…ƒæ ¼çš„æ³¨é‡Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 å•æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "è®­ç»ƒå•ä¸ªæŒ‡å®šçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•æ¨¡å‹è®­ç»ƒç¤ºä¾‹\n",
    "from main import run_all_experiments\n",
    "\n",
    "# é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯é€‰ï¼š'MobileNetV2', 'MobileNetV2_CBAM', 'VGG16', 'DenseNet121', 'EfficientNetB0'ï¼‰\n",
    "selected_model = 'MobileNetV2_CBAM'  # å¸¦CBAMæ³¨æ„åŠ›æœºåˆ¶çš„MobileNetV2\n",
    "\n",
    "print(f\"\\nè®­ç»ƒæ¨¡å‹: {selected_model}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    results, models, histories = run_all_experiments(\n",
    "        data_dir='./data/garbage_classification',\n",
    "        epochs=20,  # å¯è°ƒæ•´epochæ•°\n",
    "        models_to_compare=[selected_model]\n",
    "    )\n",
    "    print(f\"\\nâœ… {selected_model} è®­ç»ƒå®Œæˆï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è®­ç»ƒå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ç»“æœè¯„ä¼°\n",
    "\n",
    "æŸ¥çœ‹è®­ç»ƒç»“æœå’Œå¯è§†åŒ–å›¾è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºè®­ç»ƒç»“æœ\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# è¯»å–æ¨¡å‹å¯¹æ¯”ç»“æœ\n",
    "comparison_file = './outputs/model_comparison.csv'\n",
    "if os.path.exists(comparison_file):\n",
    "    df = pd.read_csv(comparison_file)\n",
    "    print(\"\\nğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœï¼š\")\n",
    "    print(\"=\" * 70)\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°å¯¹æ¯”ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ\")\n",
    "\n",
    "# æ˜¾ç¤ºæœ€ä½³æ¨¡å‹\n",
    "if os.path.exists(comparison_file):\n",
    "    df = pd.read_csv(comparison_file)\n",
    "    best_acc = df.loc[df['accuracy'].idxmax()]\n",
    "    best_speed = df.loc[df['inference_time_ms'].idxmin()]\n",
    "    \n",
    "    print(\"\\nğŸ† æœ€ä½³å‡†ç¡®ç‡æ¨¡å‹ï¼š\")\n",
    "    print(f\"  æ¨¡å‹: {best_acc['model_name']}\")\n",
    "    print(f\"  å‡†ç¡®ç‡: {best_acc['accuracy']:.4f}\")\n",
    "    print(f\"  F1åˆ†æ•°: {best_acc['f1_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nâš¡ æœ€å¿«æ¨ç†é€Ÿåº¦æ¨¡å‹ï¼š\")\n",
    "    print(f\"  æ¨¡å‹: {best_speed['model_name']}\")\n",
    "    print(f\"  æ¨ç†æ—¶é—´: {best_speed['inference_time_ms']:.2f} ms\")\n",
    "    print(f\"  FPS: {best_speed['fps']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºæ··æ·†çŸ©é˜µå’Œè®­ç»ƒæ›²çº¿\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# æŸ¥æ‰¾æ‰€æœ‰è¾“å‡ºå›¾ç‰‡\n",
    "output_dirs = [d for d in os.listdir('./outputs') if os.path.isdir(os.path.join('./outputs', d))]\n",
    "\n",
    "for model_dir in output_dirs:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"æ¨¡å‹: {model_dir}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model_path = os.path.join('./outputs', model_dir)\n",
    "    \n",
    "    # æ˜¾ç¤ºæ··æ·†çŸ©é˜µ\n",
    "    cm_path = os.path.join(model_path, 'confusion_matrix.png')\n",
    "    if os.path.exists(cm_path):\n",
    "        print(\"\\nğŸ“Š æ··æ·†çŸ©é˜µï¼š\")\n",
    "        display(IPImage(filename=cm_path))\n",
    "    \n",
    "    # æ˜¾ç¤ºè®­ç»ƒæ›²çº¿\n",
    "    tc_path = os.path.join(model_path, 'training_curves.png')\n",
    "    if os.path.exists(tc_path):\n",
    "        print(\"\\nğŸ“ˆ è®­ç»ƒæ›²çº¿ï¼š\")\n",
    "        display(IPImage(filename=tc_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. æ¶ˆèå®éªŒï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "éªŒè¯CBAMæ³¨æ„åŠ›æœºåˆ¶å’ŒFocal Lossçš„è´¡çŒ®\n",
    "\n",
    "**âš ï¸ æ³¨æ„ï¼šæ¶ˆèå®éªŒéœ€è¦è®­ç»ƒ4ä¸ªæ¨¡å‹ï¼Œè€—æ—¶è¾ƒé•¿**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¶ˆèå®éªŒ - å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥è¿è¡Œ\n",
    "# from main import run_ablation_study\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"å¼€å§‹æ¶ˆèå®éªŒ\")\n",
    "# print(\"é…ç½®ï¼š\")\n",
    "# print(\"  1. Baseline (æ— CBAMï¼Œæ— Focal Loss)\")\n",
    "# print(\"  2. + CBAM (ä»…CBAM)\")\n",
    "# print(\"  3. + Focal Loss (ä»…Focal Loss)\")\n",
    "# print(\"  4. + CBAM + Focal Loss (å®Œæ•´æå‡ºæ–¹æ³•)\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# try:\n",
    "#     ablation_results = run_ablation_study(\n",
    "#         data_dir='./data/garbage_classification',\n",
    "#         epochs=20\n",
    "#     )\n",
    "#     print(\"\\nâœ… æ¶ˆèå®éªŒå®Œæˆï¼\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nâŒ æ¶ˆèå®éªŒå¤±è´¥: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "print(\"æç¤ºï¼šå¦‚éœ€è¿è¡Œæ¶ˆèå®éªŒï¼Œè¯·å–æ¶ˆä¸Šæ–¹å•å…ƒæ ¼çš„æ³¨é‡Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ä¸‹è½½ç»“æœ\n",
    "\n",
    "æ‰“åŒ…å¹¶ä¸‹è½½æ‰€æœ‰å®éªŒç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“åŒ…ç»“æœæ–‡ä»¶\n",
    "!zip -r experiment_results.zip ./outputs ./saved_models ./logs\n",
    "\n",
    "print(\"\\nâœ… ç»“æœå·²æ‰“åŒ…ä¸º experiment_results.zip\")\n",
    "print(\"\\nåŒ…å«å†…å®¹ï¼š\")\n",
    "print(\"  ğŸ“ outputs/ - å¯è§†åŒ–å›¾è¡¨å’Œè¯„ä¼°ç»“æœ\")\n",
    "print(\"  ğŸ“ saved_models/ - è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶\")\n",
    "print(\"  ğŸ“ logs/ - TensorBoardæ—¥å¿—\")\n",
    "\n",
    "# ä¸‹è½½ç»“æœ\n",
    "from google.colab import files\n",
    "files.download('experiment_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‰ å®éªŒå®Œæˆï¼\n",
    "\n",
    "### ğŸ“ ç»“æœè¯´æ˜\n",
    "\n",
    "æœ¬å®éªŒå¯¹æ¯”äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åƒåœ¾åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼š\n",
    "\n",
    "**æ ‡å‡†æ¨¡å‹ï¼š**\n",
    "- **MobileNetV2**: è½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆç§»åŠ¨ç«¯éƒ¨ç½²\n",
    "- **VGG16**: ç»å…¸æ·±åº¦ç½‘ç»œ\n",
    "- **DenseNet121**: å¯†é›†è¿æ¥ç½‘ç»œ\n",
    "- **EfficientNetB0**: é«˜æ•ˆç½‘ç»œæ¶æ„\n",
    "\n",
    "**æ”¹è¿›æ¨¡å‹ï¼š**\n",
    "- **MobileNetV2_CBAM**: æ·»åŠ CBAMæ³¨æ„åŠ›æœºåˆ¶çš„MobileNetV2\n",
    "\n",
    "### ğŸ“Š å…³é”®æŒ‡æ ‡\n",
    "\n",
    "- **å‡†ç¡®ç‡ (Accuracy)**: åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹\n",
    "- **F1åˆ†æ•° (F1-Score)**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡\n",
    "- **æ¨ç†æ—¶é—´ (Inference Time)**: å•å¼ å›¾ç‰‡çš„å¤„ç†æ—¶é—´\n",
    "- **æ¨¡å‹å¤§å° (Model Size)**: æ¨¡å‹æ–‡ä»¶å¤§å°\n",
    "\n",
    "### ğŸ” è¿›ä¸€æ­¥æ¢ç´¢\n",
    "\n",
    "1. **TensorBoardå¯è§†åŒ–**: \n",
    "   ```python\n",
    "   %load_ext tensorboard\n",
    "   %tensorboard --logdir logs/\n",
    "   ```\n",
    "\n",
    "2. **æ¨¡å‹é¢„æµ‹ç¤ºä¾‹**:\n",
    "   ```python\n",
    "   from tensorflow.keras.models import load_model\n",
    "   model = load_model('./saved_models/MobileNetV2_final.keras')\n",
    "   # è¿›è¡Œé¢„æµ‹...\n",
    "   ```\n",
    "\n",
    "3. **æŸ¥çœ‹è¯¦ç»†è¯„ä¼°æŠ¥å‘Š**:\n",
    "   - æ··æ·†çŸ©é˜µ: `outputs/<model_name>/confusion_matrix.png`\n",
    "   - è®­ç»ƒæ›²çº¿: `outputs/<model_name>/training_curves.png`\n",
    "   - æ•°å€¼ç»“æœ: `outputs/<model_name>/results.json`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š å‚è€ƒèµ„æ–™\n",
    "\n",
    "- [MobileNetV2 è®ºæ–‡](https://arxiv.org/abs/1801.04381)\n",
    "- [CBAM è®ºæ–‡](https://arxiv.org/abs/1807.06521)\n",
    "- [Focal Loss è®ºæ–‡](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "---\n",
    "\n",
    "**å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®GitHubä»“åº“æˆ–æäº¤Issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## é™„å½•ï¼šé«˜çº§åŠŸèƒ½\n",
    "\n",
    "### A. æµ‹è¯•å•å¼ å›¾ç‰‡é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å•å¼ å›¾ç‰‡é¢„æµ‹\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model_path = './saved_models/MobileNetV2_final.keras'  # ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = load_model(model_path, custom_objects={'FocalLoss': lambda **kwargs: None}, compile=False)\n",
    "    \n",
    "    # ç±»åˆ«åç§°\n",
    "    class_names = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', \n",
    "                   'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "    \n",
    "    # ä¸Šä¼ æµ‹è¯•å›¾ç‰‡\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for fn in uploaded.keys():\n",
    "        # åŠ è½½å›¾ç‰‡\n",
    "        img = image.load_img(fn, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = class_names[np.argmax(predictions)]\n",
    "        confidence = np.max(predictions) * 100\n",
    "        \n",
    "        # æ˜¾ç¤ºç»“æœ\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"é¢„æµ‹ç±»åˆ«: {predicted_class}\\nç½®ä¿¡åº¦: {confidence:.2f}%\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.barh(class_names, predictions[0])\n",
    "        plt.xlabel('æ¦‚ç‡')\n",
    "        plt.title('å„ç±»åˆ«é¢„æµ‹æ¦‚ç‡')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\")\n",
    "    print(\"è¯·å…ˆè¿è¡Œè®­ç»ƒä»£ç \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. TensorBoardå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯åŠ¨TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. æ¨¡å‹æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ææ¨¡å‹å¤æ‚åº¦å’Œæ¨ç†é€Ÿåº¦\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comparison_file = './outputs/model_comparison.csv'\n",
    "if os.path.exists(comparison_file):\n",
    "    df = pd.read_csv(comparison_file)\n",
    "    \n",
    "    # å‡†ç¡®ç‡ vs æ¨¡å‹å¤§å°\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(df['model_size_mb'], df['accuracy'], s=100, alpha=0.6)\n",
    "    for i, model in enumerate(df['model_name']):\n",
    "        plt.annotate(model, (df['model_size_mb'][i], df['accuracy'][i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    plt.xlabel('æ¨¡å‹å¤§å° (MB)')\n",
    "    plt.ylabel('å‡†ç¡®ç‡')\n",
    "    plt.title('å‡†ç¡®ç‡ vs æ¨¡å‹å¤§å°')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # å‡†ç¡®ç‡ vs æ¨ç†æ—¶é—´\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(df['inference_time_ms'], df['accuracy'], s=100, alpha=0.6, color='orange')\n",
    "    for i, model in enumerate(df['model_name']):\n",
    "        plt.annotate(model, (df['inference_time_ms'][i], df['accuracy'][i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    plt.xlabel('æ¨ç†æ—¶é—´ (ms)')\n",
    "    plt.ylabel('å‡†ç¡®ç‡')\n",
    "    plt.title('å‡†ç¡®ç‡ vs æ¨ç†æ—¶é—´')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_tradeoff_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… æ€§èƒ½åˆ†æå›¾å·²ä¿å­˜ä¸º model_tradeoff_analysis.png\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°å¯¹æ¯”ç»“æœæ–‡ä»¶\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
